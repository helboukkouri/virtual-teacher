#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    pip-compile requirements.in
#
accelerate==0.29.2
    # via -r requirements.in
asttokens==2.4.1
    # via stack-data
bitsandbytes==0.43.1
    # via -r requirements.in
certifi==2024.2.2
    # via requests
cffi==1.16.0
    # via sounddevice
charset-normalizer==3.3.2
    # via requests
decorator==5.1.1
    # via
    #   ipdb
    #   ipython
exceptiongroup==1.2.0
    # via ipython
executing==2.0.1
    # via stack-data
filelock==3.13.4
    # via
    #   huggingface-hub
    #   torch
    #   transformers
    #   triton
fsspec==2024.3.1
    # via
    #   huggingface-hub
    #   torch
huggingface-hub==0.22.2
    # via
    #   accelerate
    #   tokenizers
    #   transformers
idna==3.7
    # via requests
ipdb==0.13.13
    # via -r requirements.in
ipython==8.23.0
    # via ipdb
jedi==0.19.1
    # via ipython
jinja2==3.1.3
    # via torch
markupsafe==2.1.5
    # via jinja2
matplotlib-inline==0.1.7
    # via ipython
mpmath==1.3.0
    # via sympy
networkx==3.3
    # via torch
numpy==1.26.4
    # via
    #   -r requirements.in
    #   accelerate
    #   bitsandbytes
    #   transformers
nvidia-cublas-cu12==12.1.3.1
    # via
    #   nvidia-cudnn-cu12
    #   nvidia-cusolver-cu12
    #   torch
nvidia-cuda-cupti-cu12==12.1.105
    # via torch
nvidia-cuda-nvrtc-cu12==12.1.105
    # via torch
nvidia-cuda-runtime-cu12==12.1.105
    # via torch
nvidia-cudnn-cu12==8.9.2.26
    # via torch
nvidia-cufft-cu12==11.0.2.54
    # via torch
nvidia-curand-cu12==10.3.2.106
    # via torch
nvidia-cusolver-cu12==11.4.5.107
    # via torch
nvidia-cusparse-cu12==12.1.0.106
    # via
    #   nvidia-cusolver-cu12
    #   torch
nvidia-nccl-cu12==2.19.3
    # via torch
nvidia-nvjitlink-cu12==12.4.127
    # via
    #   nvidia-cusolver-cu12
    #   nvidia-cusparse-cu12
nvidia-nvtx-cu12==12.1.105
    # via torch
packaging==24.0
    # via
    #   accelerate
    #   huggingface-hub
    #   transformers
parso==0.8.4
    # via jedi
pexpect==4.9.0
    # via ipython
prompt-toolkit==3.0.43
    # via ipython
psutil==5.9.8
    # via accelerate
ptyprocess==0.7.0
    # via pexpect
pure-eval==0.2.2
    # via stack-data
pycparser==2.22
    # via cffi
pygments==2.17.2
    # via ipython
pyyaml==6.0.1
    # via
    #   accelerate
    #   huggingface-hub
    #   transformers
regex==2024.4.16
    # via transformers
requests==2.31.0
    # via
    #   huggingface-hub
    #   transformers
safetensors==0.4.3
    # via
    #   accelerate
    #   transformers
sentencepiece==0.2.0
    # via -r requirements.in
six==1.16.0
    # via asttokens
sounddevice==0.4.6
    # via -r requirements.in
stack-data==0.6.3
    # via ipython
sympy==1.12
    # via torch
tokenizers==0.15.2
    # via transformers
tomli==2.0.1
    # via ipdb
torch==2.2.2
    # via
    #   -r requirements.in
    #   accelerate
    #   bitsandbytes
    #   torchaudio
torchaudio==2.2.2
    # via -r requirements.in
tqdm==4.66.2
    # via
    #   huggingface-hub
    #   transformers
traitlets==5.14.2
    # via
    #   ipython
    #   matplotlib-inline
transformers==4.39.3
    # via -r requirements.in
triton==2.2.0
    # via torch
typing-extensions==4.11.0
    # via
    #   huggingface-hub
    #   ipython
    #   torch
urllib3==2.2.1
    # via requests
wcwidth==0.2.13
    # via prompt-toolkit
